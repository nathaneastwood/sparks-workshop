# `sparklyr`

## Installing a Local Version of Spark

We connect to Spark and call Scala methods using the `sparklyr` package. To install a local version of Spark we can run.

```{r install_spark}
spark_install()
```

To check the installation, run:

```{r check_install, eval = TRUE}
spark_install_find()
```

## Connecting to Spark

The `sparklyr` package has a function called `spark_connect()` which we can configure to connect to a local instance of Spark or a server instance. Here we show some code for connecting locally.

```{r connect, eval = TRUE}
config <- spark_config()
config$sparklyr.gateway.address <- "127.0.0.1"
sc <- spark_connect(master = "local", version = "2.2.0", config = config)
```

### Data Types

`sparklyr` has a function named [`sdf_schema()`](https://www.rdocumentation.org/packages/sparklyr/versions/0.7.0/topics/sdf_schema) for exploring the columns of a `tibble`[^footnote] on the R side. The return value is a list, and each element is a list with two elements, containing the name and data type of each column.

[^footnote]: A `tibble` is a `data.frame` that provides stricter checking and better formatting than the traditional `data.frame`.

```{r sdf_schema, eval = TRUE}
data <- spark_read_json(sc, name = "data", "../../data_raw/Melt.json") %>% 
  spark_dataframe()
sdf_schema(data)
```

Here is a comparison of how R data types map to Spark data types. Other data types are not currently supported by `sparklyr`.

| R type | Spark type |
---------|-------------
| logical | BooleanType |
| numeric | DoubleType |
| integer | IntegerType |
| character | StringType |
| list | ArrayType |

`sparklyr` [doesn't currently have the ability](https://github.com/rstudio/sparklyr/issues/1324) to pass over more complex data types such as a `List[String]`. 

### Using other data types

When passing an R `list` over to Scala, we get a Scala `ArrayType` and there is no current way to send a Scala `List` from R using `sparklyr`. However, some of our Scala functions require `List` inputs. Potential solutions to this issue are:

1. Use `Seq` instead of `List` as the input type since `Array` has also the `Seq` trait in Scala, so everything works out-of-the-box.
2. Use overloading, which allows us to define methods of same name but having different parameters or data types, though this [has issues](https://stackoverflow.com/questions/2510108/why-avoid-method-overloading). For an example of how this works, see [this link](https://www.javatpoint.com/scala-method-overloading).
3. Define a new Scala method for the same class that is called from R, which effectively invokes the `toList` function on the `ArrayType` and then calls the existing Scala method.

We can create Java `ArrayList`s in the Spark environment using the following code:

```{r invoke_new}
# map some R vector `x` to a java ArrayList
al <- invoke_new(sc, "java.util.ArrayList")
lapply(x, FUN = function(y){invoke(al, "add", y)})
```

Note we don't need to reassign the results of the `lapply` because it is adding values to the Scala `List` in the JVM. We can then convert this code to a Scala `List` using:

```{r invoke_list}
invoke_static(sc, "scala.collection.JavaConversions", "asScalaBuffer", al) %>%
  invoke("toSeq") %>%
  invoke("toList")
```

## Reading Data
You can copy R data frames into Spark using the `dplyr::copy_to` function. (More typically, though, you’ll read data within the Spark cluster using the `spark_read` family of functions.)

```{r copy_to}
library(dplyr) 
iris_tbl <- copy_to(sc, iris)
iris_tbl
```

## Calling Scala Methods

There are three main functions for calling methods within Spark.

```{r spark_invoke}
invoke_new()    # create new scala objects
invoke_static() # call static methods from scala
invoke()        # call methods from scala object
```

## The Jar (`inst/java/`)

If you plan on having the Jar file within your R package, such that it is self contained, within the `inst` folder, you will need a `java` folder; this is where the Jar lives. We register this Jar using the `dependencies.R` file which is shown below.

```{r dependencies}
spark_dependencies <- function(spark_version, scala_version, ...) {
  sparklyr::spark_dependency(
    jars = c(
      system.file(
        "java/sparkts-0.4.0-SNAPSHOT-jar-with-dependencies.jar",
        package = "sparkts"
      )
    )
  )
}

#' @import sparklyr
.onLoad <- function(libname, pkgname) {
  sparklyr::register_extension(pkgname)
}
```

The `.onLoad` function is ran whenever `sparkts` is loaded. Registering an extension package will result in the package being automatically scanned for spark dependencies when a connection to Spark is created. Packages should typically register their extensions in their `.onLoad` hook – this ensures that their extensions are registered when their namespaces are loaded. Here we define the Jar by providing its path within the `sparklyr::spark_dependency` function.