---
title: "Questions"
output: 
  html_notebook:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = FALSE
)
```

# Question 1

Write an R function to call the method `melt1` defined in the `Melt.Scala` file.

```{r sdf_melt}
sdf_melt <- function(sc, data, id_variables, value_variables, variable_name,
                     value_name) {

  invoke_static(
    sc = sc,
    class = "com.ons.sml.businessMethods.methods.Melt",
    method = "melt",
    df = data
  ) %>%
    invoke(
      method = "melt1",
      dfIn = data,
      id_vars = scala_seq(sc, id_variables),
      value_vars = scala_seq(sc, value_variables),
      var_name = variable_name,
      value_name = value_name
    )
}
```

# Question 2

Add some documentation to your function using Roxygen.

```{r documentation}
#' Call the melt method
#'
#' This method will take a sequence of column names (strings) and unpivots them
#' into two columns, the "variable_name" and its values.
#'
#' @param sc A \code{spark_connection}.
#' @param data A \code{jobj}: the Spark \code{DataFrame} on which to perform the
#'   function.
#' @param id_variables list(string). Column(s) which are used as unique
#'   identifiers.
#' @param value_variables list(string). Column(s) which are being unpivoted.
#' @param variable_name c(string). The name of a new column, which holds all
#'   the \code{value_variables} names, defaulted to "variable".
#' @param value_name c(string). The name of a new column, which holds all the
#'   values of \code{value_variables} column(s). Defaults to "value".
#'
#' @return Returns a \code{jobj}
#'
#' @examples
#' \dontrun{
#' # Set up a spark connection
#' sc <- spark_connect(master = "local", version = "2.2.0")
#'
#' # Extract some data
#' melt_data <- spark_read_json(
#'   sc,
#'   "melt_data",
#'   path = system.file(
#'     "data_raw/Melt.json",
#'     package = "sparkts"
#'   )
#' ) %>%
#'   spark_dataframe()
#'
#' # Call the method
#' p <- sdf_melt(
#'   sc = sc, data = melt_data, id_variables = c("identifier", "date"),
#'   value_variables = c("two", "one", "three", "four"),
#'   variable_name = "variable", value_name = "turnover"
#' )
#'
#' #' # Return the data to R
#' p %>% dplyr::collect()
#'
#' spark_disconnect(sc = sc)
#' }
#'
#' @export
sdf_melt <- function(sc, data, id_variables, value_variables, variable_name,
                     value_name) {

  invoke_static(
    sc = sc,
    class = "com.ons.sml.businessMethods.methods.Melt",
    method = "melt",
    df = data
  ) %>%
    invoke(
      method = "melt1",
      dfIn = data,
      id_vars = scala_seq(sc, id_variables),
      value_vars = scala_seq(sc, value_variables),
      var_name = variable_name,
      value_name = value_name
    )
}
```

# Question 3

Create the `.Rd` file.

```{r create_docs}
devtools::document()
```

# Question 4

Set up the test structure.

```{r test_str}
devtools::use_testthat()
```

# Question 5

Create a test file.

```{r create_test}
devtools::use_test("test-sdf_melt")
```

# Question 6

Write a test for your function.

```{r test}
context("Test the sdf_melt function")

sc <- sparklyr::spark_connect(
  master = "local", 
  version = "2.2.0", 
  config = list(sparklyr.gateway.address = "127.0.0.1")
)

test_that("Ensure the melt method returns the expected output", {

  # Read in the data
  melt_std_data <- sparklyr::spark_read_json(
    sc,
    "melt_std_data",
    path = system.file(
      "data_raw/Melt.json",
      package = "sparkts"
    )
  ) %>%
    sparklyr::spark_dataframe()

  # Instantiate the class
  output <- sdf_melt(
    sc = sc,
    data = melt_std_data,
    id_variables = c("identifier", "date"),
    value_variables = c("two", "one", "three", "four"),
    variable_name = "variable",
    value_name = "turnover"
  ) %>%
    dplyr::collect()

  # Test the expectation
  expect_identical(
    output,
    expected_sdf_melt
  )

})
```
