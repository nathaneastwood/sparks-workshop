---
title: "sparkts Workshop"
output: 
  html_document:
    number_sections: true
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  eval = FALSE,
  collapse = TRUE,
  comment = "#"
)
library(sparklyr)
```

# Introduction

This half-day workshop is intended to be a brief look into the core components of R package building and using `sparklyr`. It is very heavily influenced by the excellent [R Packages Book](http://r-pkgs.had.co.nz) by Hadley Wickham. In this workshop you will learn how to write reproducible R code in the form of functions and how to test and document them.

# Writing functions (`R/`)

Everything in R is an object. Everything we do to an object is a function.

![Figure 1: How to write a quick function](https://media.giphy.com/media/C9E08n1ssdGuudtQFk/giphy.gif)

## Getting Help

You can get help in R using either the `?` function or the `help()` function.

```{r help}
?print
```

You can search for things within R using `??` or `help.search()`.

```{r help_search}
??print
```

# `sparklyr`
## Installing a Local Version of Spark

We connect to Spark and call Scala methods using the `sparklyr` package. To install a local version of Spark we can run.

```{r install_spark}
library(sparklyr)
spark_install()
```

To check the installation, run:

```{r check_install, eval = TRUE}
spark_install_find()
```

## Connecting to Spark

The `sparklyr` package has a function called `spark_connect()` which we can configure to connect to a local instance of Spark or a server instance. Here we show some code for connecting locally.

```{r connect}
config <- spark_config()
config$sparklyr.gateway.address <- "127.0.0.1"
sc <- spark_connect(master = "local", version = "2.2.0", config = config)
```

### Data Types

sparklyr has a function named [`sdf_schema()`](https://www.rdocumentation.org/packages/sparklyr/versions/0.7.0/topics/sdf_schema) for exploring the columns of a tibble on the R side. The return value is a list, and each element is a list with two elements, containing the name and data type of each column.

Here is a comparison of how R data types map to Spark data types. Other data types are not currently supported by `sparklyr`.

| R type | Spark type |
---------|-------------
| logical | BooleanType |
| numeric | DoubleType |
| integer | IntegerType |
| character | StringType |
| list | ArrayType |

`sparklyr` [doesn't currently have the ability](https://github.com/rstudio/sparklyr/issues/1324) to pass over more complex data types such as a `List[String]`. 

### Using other data types

When passing an R `list` over to Scala, we get a Scala `ArrayType` and there is no current way to send a Scala `List` from R using `sparklyr`. However, some of our Scala functions require `List` inputs. Potential solutions to this issue are:

1. Use `Seq` instead of `List` as the input type since `Array` has also the `Seq` trait in Scala, so everything works out-of-the-box.
2. Use overloading, which allows us to define methods of same name but having different parameters or data types, though this [has issues](https://stackoverflow.com/questions/2510108/why-avoid-method-overloading). For an example of how this works, see [this link](https://www.javatpoint.com/scala-method-overloading).
3. Define a new Scala method for the same class that is called from R, which effectively invokes the `toList` function on the `ArrayType` and then calls the existing Scala method.

We can create Java `ArrayList`s in the Spark environment using the following code:

```r
# map some R vector `x` to a java ArrayList
al <- invoke_new(sc, "java.util.ArrayList")
lapply(x, FUN = function(y){invoke(al, "add", y)})
```

Note we don't need to reassign the results of the `lapply` because it is adding values to the Scala `List` in the JVM. We can then convert this code to a Scala `List` using:

```r
invoke_static(sc, "scala.collection.JavaConversions", "asScalaBuffer", al) %>%
  invoke("toSeq") %>%
  invoke("toList")
```

## Reading Data
You can copy R data frames into Spark using the `dplyr::copy_to` function. (More typically, though, you’ll read data within the Spark cluster using the `spark_read` family of functions.)

```{r copy_to}
library(dplyr) 
iris_tbl <- copy_to(sc, iris)
iris_tbl
```

## Calling Scala Methods

There are three main functions for calling methods within Spark.

```{r spark_invoke}
invoke_new()    # create new scala objects
invoke_static() # call static methods from scala
invoke()        # call methods from scala object
```

## Testing Your Code

You can use `devtools::load_all()` to simulate building and loading your package. This is useful for when you have written some new code or a new feature and you want to interactively run it.

### Behavior-driven development (BDD)

To implement BDD in R, one should use the `testthat::describe()` function. Examples can be seen in `?testthat::describe`.

More robust testing is done using the `testthat` package, however.

# Package metadata (`DESCRIPTION`)

# Object Documentation (`man/`)

Documentation in R is saved as `.Rd` files. These live separately to our R functions which live in `.R` files. However we can generate our `.Rd` files using an R package called `roxygen2` which parses the `.R` files.

We include a `roxygen` header above the function definition. Each line of the `roxygen` header starts with the symbols `#'`. This is known as a `roxygen` comment. Following this we use special tags to indicate a particular component of the help file. However the first three elements have special meaning. The first three paragraphs of the header are treated as:

* The title of the help page (short, one sentence)
* The description for the help page (brief description of the function)
* The details section which can provide much more information about the function, what it implements etc. 

Following these lines we must use the tags to distinguish elements. These tags all start with an `@` symbol. Some tags and their uses are:

| Tag | Purpose |
| --- | ------- |
| @param | Identify each of the function arguments and the corresponding help text |
| @return | Detail the output of the function |
| @author | Who wrote this function? |
| @seealso | Other functions that the user should also look at the help documentation for |
| @examples	| Code examples of running the function |
| @import/@importFrom | Indicate a package or function within a package to be imported |
| @export | Indicate that this function should be exported (i.e. made visible to the end user) |

The following is an example of how this might look for an example function.

```{r roxygen_ex}
#' Sample from a dataset	 
#'
#' This function has been designed to sample from the rows of a two 
#' dimensional data set returning all columns of the sampled rows.
#'
#' @param data The matrix or data.frame from which rows are to be sampled.
#' @param size The number of samples to take.
#' @param replace Should values be replaced? By default takes the value TRUE.
#' @param ... Any other parameters to be passed to the sample function.
#'
#' @return Returns a dataset of the same type as the input data with \code{size} 
#' rows.
#'
#' @author Nathan Eastwood <neastwood@@mango-solutions.com>
#' 
#' @examples
#' sampleFromData(airquality, 100)
#'
#' @export
sampleFromData <- function(data, size, replace = TRUE, ...) {
...
}
```

# Vignettes (`vignettes/`)

# Testing (`tests/`)

## Code Coverage

# Namespaces (`NAMESPACE`)

As the name suggests, `NAMESPACE` provides a "space" for "names". It provides a context for looking up the value of an object associated with a name. When loading different packages, sometimes they can have functions with the same names. So how does R know which function to use? Take for example, the `summarize` functions in the `Hmisc` and `plyr` packages. 

A namespace make your package self-contained in two ways: the imports and the exports. The imports define how a function in one package finds a function in another. The exports help you avoid conflicts with other packages by specifying which functions are available outside of your package (internal functions are available only within your package and can’t easily be used by another package). 

You can, however, explicitly refer to specific functions: `Hmisc::summarize()` and `plyr::summarize()`. All the `::` function does is explicitly load a function from a given package.

# Installaed Files (`inst/`)

The `inst` folder is designed to hold any raw data files you may have or other files the package uses such as the Jar. 

## Jar (`java/`)

If you plan on having the Jar file within your R package, such that it is self contained, within the `inst` folder, you will need a `java` folder; this is where the Jar lives. We register this Jar using the `dependencies.R` file which is shown below.

```{dependencies}
spark_dependencies <- function(spark_version, scala_version, ...) {
  sparklyr::spark_dependency(
    jars = c(
      system.file(
        "java/sparkts-0.4.0-SNAPSHOT-jar-with-dependencies.jar",
        package = "sparkts"
      )
    )
  )
}

#' @import sparklyr
.onLoad <- function(libname, pkgname) {
  sparklyr::register_extension(pkgname)
}
```

The `.onLoad` function is ran whenever `sparkts` is loaded. Registering an extension package will result in the package being automatically scanned for spark dependencies when a connection to Spark is created. Packages should typically register their extensions in their `.onLoad` hook – this ensures that their extensions are registered when their namespaces are loaded. Here we define the Jar by providing its path within the `sparklyr::spark_dependency` function.

## Raw Data (`data_raw/`)

The `sparkts` package can use the raw JSON files used within the tests by storing them in a `data_raw` folder within the `inst` folder. You can set up the raw data folder using:

```{data_raw}
devtools::use_data_raw()
```

# Linting

Generally I have stuck to the [tidyverse coding standards](http://style.tidyverse.org). You can automatically format your code using two packages.

## `lintr`

The `sparkts` package automatically runs [`lintr`](https://github.com/jimhester/lintr) to check for style errors during the test phase of the package check or when running `devtools::test()`. You can install `lintr` with:

```{r lint}
remotes::install_github("jimhester/lintr")
```

Then to run the linter run

```{r run_lint}
lintr::lint_package()
```

## `styler`

The goal of `styler` is to provide non-invasive pretty-printing of R source code while adhering to the [tidyverse formatting rules](http://style.tidyverse.org). You can run `styler` on your entire package using

```{r style_pkg}
styler::style_pkg()
```

Or you can style an individual file using the RStudio Addin.

![how to use styler to styler a file](https://raw.githubusercontent.com/lorenzwalthert/some_raw_data/master/styler_0.1.gif)

# Building, Checking and Installing the Package

Once you are at the point
